root@nl8wk3a64x:/notebooks# python3 DeBERTa_with_data_augmentation.py 
Traceback (most recent call last):
  File "/notebooks/DeBERTa_with_data_augmentation.py", line 7, in <module>
    import nlpaug.augmenter.word as naw
ModuleNotFoundError: No module named 'nlpaug'
root@nl8wk3a64x:/notebooks# pip install nlpaug
Collecting nlpaug
  Downloading nlpaug-1.1.11-py3-none-any.whl (410 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 410.5/410.5 kB 39.9 MB/s eta 0:00:00
Requirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.9/dist-packages (from nlpaug) (1.23.4)
Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from nlpaug) (1.5.0)
Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.9/dist-packages (from nlpaug) (2.28.2)
Requirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.9/dist-packages (from nlpaug) (4.5.1)
Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from gdown>=4.0.0->nlpaug) (3.9.0)
Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.9/dist-packages (from gdown>=4.0.0->nlpaug) (4.11.1)
Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from gdown>=4.0.0->nlpaug) (4.64.1)
Requirement already satisfied: six in /usr/lib/python3/dist-packages (from gdown>=4.0.0->nlpaug) (1.14.0)
Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.2.0->nlpaug) (2.8.2)
Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.2.0->nlpaug) (2022.7.1)
Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.22.0->nlpaug) (2019.11.28)
Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests>=2.22.0->nlpaug) (2.1.1)
Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.22.0->nlpaug) (2.8)
Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.22.0->nlpaug) (1.26.14)
Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.3.2.post1)
Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.9/dist-packages (from requests>=2.22.0->nlpaug) (1.7.1)
Installing collected packages: nlpaug
Successfully installed nlpaug-1.1.11
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv
root@nl8wk3a64x:/notebooks# python3 DeBERTa_with_data_augmentation.py 
cuda:0
Starting data augmentation
Downloading tokenizer_config.json: 100%|█████████████████████████████████████████████████████████████████████████████████| 28.0/28.0 [00:00<00:00, 52.1kB/s]
Downloading config.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 483/483 [00:00<00:00, 679kB/s]
Downloading vocab.txt: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 226k/226k [00:00<00:00, 2.35MB/s]
Downloading tokenizer.json: 100%|████████████████████████████████████████████████████████████████████████████████████████| 455k/455k [00:00<00:00, 1.99MB/s]
Downloading pytorch_model.bin: 100%|█████████████████████████████████████████████████████████████████████████████████████| 256M/256M [00:02<00:00, 98.2MB/s]
Data augmentation finished
Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias']
- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.weight', 'classifier.bias', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.21.3",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 8644
  Num Epochs = 10
  Instantaneous batch size per device = 8
  Total train batch size (w. parallel, distributed & accumulation) = 8
  Gradient Accumulation steps = 1
  Total optimization steps = 10810
Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
wandb: (1) Create a W&B account
wandb: (2) Use an existing W&B account
wandb: (3) Don't visualize my results
wandb: Enter your choice: 3
wandb: You chose 'Don't visualize my results'
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
wandb: Tracking run with wandb version 0.13.4
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
  0%|                                                                                                                             | 0/10810 [00:00<?, ?it/s]/notebooks/DeBERTa_with_data_augmentation.py:67: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
{'loss': 0.2757, 'learning_rate': 9.537465309898243e-06, 'epoch': 0.46}                                                                                     
{'loss': 0.23, 'learning_rate': 9.074930619796487e-06, 'epoch': 0.93}                                                                                       
 10%|███████████▏                                                                                                    | 1081/10810 [11:28<1:29:41,  1.81it/s]***** Running Evaluation *****
  Num examples = 1675
  Batch size = 8
{'eval_loss': 0.25089624524116516, 'eval_accuracy': 0.9176119402985075, 'eval_f1 score': 0.10389610389610389, 'eval_runtime': 44.0192, 'eval_samples_per_second': 38.052, 'eval_steps_per_second': 4.771, 'epoch': 1.0}                                                                                                 
 10%|███████████▏                                                                                                    | 1081/10810 [12:12<1:29:41,  1.81it/sSaving model checkpoint to Deberta/checkpoint-1081                                                                                                           
Configuration saved in Deberta/checkpoint-1081/config.json
/notebooks/DeBERTa_with_data_augmentation.py:67: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
{'loss': 0.195, 'learning_rate': 8.612395929694729e-06, 'epoch': 1.39}                                                                                      
{'loss': 0.1874, 'learning_rate': 8.14986123959297e-06, 'epoch': 1.85}                                                                                      
 20%|██████████████████████▍                                                                                         | 2162/10810 [23:44<1:19:48,  1.81it/s]***** Running Evaluation *****
  Num examples = 1675
  Batch size = 8
{'eval_loss': 0.21063341200351715, 'eval_accuracy': 0.9337313432835821, 'eval_f1 score': 0.615916955017301, 'eval_runtime': 44.0829, 'eval_samples_per_second': 37.997, 'eval_steps_per_second': 4.764, 'epoch': 2.0}                                                                                                   
 20%|██████████████████████▍                                                                                         | 2162/10810 [24:28<1:19:48,  1.81it/sSaving model checkpoint to Deberta/checkpoint-2162                                                                                                           
Configuration saved in Deberta/checkpoint-2162/config.json
/notebooks/DeBERTa_with_data_augmentation.py:67: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
{'loss': 0.1268, 'learning_rate': 7.687326549491213e-06, 'epoch': 2.31}                                                                                     
{'loss': 0.118, 'learning_rate': 7.224791859389455e-06, 'epoch': 2.78}                                                                                      
 30%|█████████████████████████████████▌                                                                              | 3243/10810 [35:59<1:09:52,  1.80it/s]***** Running Evaluation *****
  Num examples = 1675
  Batch size = 8
{'eval_loss': 0.3438858389854431, 'eval_accuracy': 0.9265671641791045, 'eval_f1 score': 0.44843049327354256, 'eval_runtime': 44.081, 'eval_samples_per_second': 37.998, 'eval_steps_per_second': 4.764, 'epoch': 3.0}                                                                                                   
 30%|█████████████████████████████████▌                                                                              | 3243/10810 [36:43<1:09:52,  1.80it/sSaving model checkpoint to Deberta/checkpoint-3243                                                                                                           
Configuration saved in Deberta/checkpoint-3243/config.json
/notebooks/DeBERTa_with_data_augmentation.py:67: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
{'loss': 0.0651, 'learning_rate': 6.762257169287697e-06, 'epoch': 3.24}                                                                                     
{'loss': 0.0458, 'learning_rate': 6.299722479185939e-06, 'epoch': 3.7}                                                                                      
 40%|█████████████████████████████████████████████▌                                                                    | 4324/10810 [48:15<59:48,  1.81it/s]***** Running Evaluation *****
  Num examples = 1675
  Batch size = 8
{'eval_loss': 0.48347008228302, 'eval_accuracy': 0.9295522388059702, 'eval_f1 score': 0.5390625, 'eval_runtime': 44.0375, 'eval_samples_per_second': 38.036, 'eval_steps_per_second': 4.769, 'epoch': 4.0}                                                                                                              
 40%|█████████████████████████████████████████████▌                                                                    | 4324/10810 [48:59<59:48,  1.81it/sSaving model checkpoint to Deberta/checkpoint-4324                                                                                                           
Configuration saved in Deberta/checkpoint-4324/config.json
/notebooks/DeBERTa_with_data_augmentation.py:67: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
{'loss': 0.0396, 'learning_rate': 5.837187789084182e-06, 'epoch': 4.16}                                                                                     
{'loss': 0.0144, 'learning_rate': 5.374653098982424e-06, 'epoch': 4.63}                                                                                     
 50%|████████████████████████████████████████████████████████                                                        | 5405/10810 [1:00:30<49:47,  1.81it/s]***** Running Evaluation *****
  Num examples = 1675
  Batch size = 8
{'eval_loss': 0.5366493463516235, 'eval_accuracy': 0.9265671641791045, 'eval_f1 score': 0.5494505494505495, 'eval_runtime': 44.0435, 'eval_samples_per_second': 38.031, 'eval_steps_per_second': 4.768, 'epoch': 5.0}                                                                                                   
 50%|████████████████████████████████████████████████████████                                                        | 5405/10810 [1:01:14<49:47,  1.81it/sSaving model checkpoint to Deberta/checkpoint-5405                                                                                                           
Configuration saved in Deberta/checkpoint-5405/config.json
/notebooks/DeBERTa_with_data_augmentation.py:67: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
{'loss': 0.0164, 'learning_rate': 4.912118408880667e-06, 'epoch': 5.09}                                                                                     
{'loss': 0.0088, 'learning_rate': 4.449583718778909e-06, 'epoch': 5.55}                                                                                     
 60%|███████████████████████████████████████████████████████████████████▏                                            | 6486/10810 [1:12:45<39:51,  1.81it/s]***** Running Evaluation *****
  Num examples = 1675
  Batch size = 8
{'eval_loss': 0.6078842878341675, 'eval_accuracy': 0.924179104477612, 'eval_f1 score': 0.5780730897009966, 'eval_runtime': 44.0574, 'eval_samples_per_second': 38.019, 'eval_steps_per_second': 4.767, 'epoch': 6.0}                                                                                                    
 60%|███████████████████████████████████████████████████████████████████▏                                            | 6486/10810 [1:13:29<39:51,  1.81it/sSaving model checkpoint to Deberta/checkpoint-6486                                                                                                           
Configuration saved in Deberta/checkpoint-6486/config.json
/notebooks/DeBERTa_with_data_augmentation.py:67: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
{'loss': 0.0188, 'learning_rate': 3.987049028677152e-06, 'epoch': 6.01}                                                                                     
{'loss': 0.0052, 'learning_rate': 3.5245143385753933e-06, 'epoch': 6.48}                                                                                    
{'loss': 0.0068, 'learning_rate': 3.0619796484736358e-06, 'epoch': 6.94}                                                                                    
 70%|██████████████████████████████████████████████████████████████████████████████▍                                 | 7567/10810 [1:25:00<29:53,  1.81it/s]***** Running Evaluation *****
  Num examples = 1675
  Batch size = 8
{'eval_loss': 0.6260086297988892, 'eval_accuracy': 0.9259701492537313, 'eval_f1 score': 0.5602836879432624, 'eval_runtime': 44.0448, 'eval_samples_per_second': 38.029, 'eval_steps_per_second': 4.768, 'epoch': 7.0}                                                                                                   
 70%|██████████████████████████████████████████████████████████████████████████████▍                                 | 7567/10810 [1:25:44<29:53,  1.81it/sSaving model checkpoint to Deberta/checkpoint-7567                                                                                                           
Configuration saved in Deberta/checkpoint-7567/config.json
/notebooks/DeBERTa_with_data_augmentation.py:67: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
{'loss': 0.0067, 'learning_rate': 2.5994449583718782e-06, 'epoch': 7.4}                                                                                     
{'loss': 0.0069, 'learning_rate': 2.1369102682701203e-06, 'epoch': 7.86}                                                                                    
 80%|█████████████████████████████████████████████████████████████████████████████████████████▌                      | 8648/10810 [1:37:15<19:55,  1.81it/s]***** Running Evaluation *****
  Num examples = 1675
  Batch size = 8
{'eval_loss': 0.6633722186088562, 'eval_accuracy': 0.9295522388059702, 'eval_f1 score': 0.572463768115942, 'eval_runtime': 44.0527, 'eval_samples_per_second': 38.023, 'eval_steps_per_second': 4.767, 'epoch': 8.0}                                                                                                    
 80%|█████████████████████████████████████████████████████████████████████████████████████████▌                      | 8648/10810 [1:37:59<19:55,  1.81it/sSaving model checkpoint to Deberta/checkpoint-8648                                                                                                           
Configuration saved in Deberta/checkpoint-8648/config.json
/notebooks/DeBERTa_with_data_augmentation.py:67: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
{'loss': 0.0004, 'learning_rate': 1.6743755781683626e-06, 'epoch': 8.33}                                                                                    
{'loss': 0.0042, 'learning_rate': 1.211840888066605e-06, 'epoch': 8.79}                                                                                     
 90%|████████████████████████████████████████████████████████████████████████████████████████████████████▊           | 9729/10810 [1:49:30<09:57,  1.81it/s]***** Running Evaluation *****
  Num examples = 1675
  Batch size = 8
{'eval_loss': 0.6846262812614441, 'eval_accuracy': 0.9295522388059702, 'eval_f1 score': 0.5203252032520325, 'eval_runtime': 44.0621, 'eval_samples_per_second': 38.015, 'eval_steps_per_second': 4.766, 'epoch': 9.0}                                                                                                   
 90%|████████████████████████████████████████████████████████████████████████████████████████████████████▊           | 9729/10810 [1:50:14<09:57,  1.81it/sSaving model checkpoint to Deberta/checkpoint-9729                                                                                                           
Configuration saved in Deberta/checkpoint-9729/config.json
/notebooks/DeBERTa_with_data_augmentation.py:67: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
{'loss': 0.0003, 'learning_rate': 7.493061979648475e-07, 'epoch': 9.25}                                                                                     
 97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▍   | 10462/10810 [1:58:03<03:41,  97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▍   | 10463/10810 [1:58:04<03:40,  97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▍   | 10464/10810 [1:58:05<03:40,  97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▍   | 10465/10810 [1:58:05<03:39,  97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▍   | 10466/10810 [1:58:06<03:38,  97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▍   | 10467/10810 [1:58:06<03:38,  97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▍   | 10468/10810 [1:58:07<03:37,  97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▍   | 10469/10810 [1:58:08<03:37,  97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▌   | 10470/10810 [1:58:08<03:36,  97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▌   | 10471/10810 [1:58:09<03:35,  97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▌   | 10472/10810 [1:58:10<03:35,  97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▌   | 10473/10810 [1:58:10<03:34,  97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▌   | 10474/10810 [1:58:11<03:33,  97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▌   | 10475/10810 [1:58:12<03:32,  97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▌   | 10476/10810 [1:58:12<03:32,  97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▌   | 10477/10810 [1:58:13<03:31,  97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▌   | 10478/10810 [1:58:13<03:31,  97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▌   | 10479/10810 [1:58:14<03:30,  97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▌   | 10480/10810 [1:58:15<03:29,  97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▌   | 10481/10810 [1:58:15<03:29,  97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▋   | 10482/10810 [1:58:16<03:28,  97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▋   | 10483/10810 [1:58:17<03:28,  97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▋   | 10484/10810 [1:58:17<03:27,  97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▋   | 10485/10810 [1:58:18<03:26,  97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▋   | 10486/10810 [1:58:19<03:26,  97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▋   | 10487/10810 [1:58:19<03:25,  97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▋   | 10488/10810 [1:58:20<03:24,  97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▋   | 10489/10810 [1:58:20<03:24,  97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▋   | 10490/10810 [1:58:21<03:23,  97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▋   | 10491/10810 [1:58:22<03:22,  97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▋   | 10492/10810 [1:58:22<03:22,  97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▋   | 10493/10810 [1:58:23<03:21,  97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▊   | 10494/10810 [1:58:24<03:20,  97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▊   | 10495/10810 [1:58:24<03:20,  97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▊   | 10496/10810 [1:58:25<03:19,  1.57it 97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▊   | 10497/10810 [1:58:26<03:19,  1.57it 97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▊   | 10498/10810 [1:58:26<03:18,  1.57it 97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▊   | 10499/10810 [1:58:27<03:17,  1.57it 97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▊   | 10500/10810 [1:58:28<03:30,  1.47it                                                                                                                                                         {'loss': 0.0016, 'learning_rate': 2.8677150786308976e-07, 'epoch': 9.71}
 97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▊   | 10500/10810 [1:58:28<03:30,  1.47it 97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▊   | 10501/10810 [1:58:28<03:26,  1.50it 97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▊   | 10502/10810 [1:58:29<03:22,  1.52it 97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▊   | 10503/10810 [1:58:30<03:19,  1.54it 97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▊   | 10504/10810 [1:58:30<03:17,  1.55it 97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▊   | 10505/10810 [1:58:31<03:16,  1.55it 97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▉   | 10506/10810 [1:58:31<03:15,  1.56it 97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▉   | 10507/10810 [1:58:32<03:13,  1.56it 97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▉   | 10508/10810 [1:58:33<03:13,  1.56it 97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▉   | 10509/10810 [1:58:33<03:11,  1.57it 97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▉   | 10510/10810 [1:58:34<03:11,  1.57it 97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▉   | 10511/10810 [1:58:35<03:10,  1.57it 97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▉   | 10512/10810 [1:58:35<03:09,  1.57it 97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▉   | 10513/10810 [1:58:36<03:09,  1.57it 97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▉   | 10514/10810 [1:58:37<03:08,  1.57it 97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▉   | 10515/10810 [1:58:37<03:07,  1.57it 97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▉   | 10516/10810 [1:58:38<03:07,  1.57it 97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▉   | 10517/10810 [1:58:38<03:06,  1.57it 97%|████████████████████████████████████████████████████████████████████████████████████████████████████████████   | 10518/10810 [1:58:39<03:05,  1.57it 97%|████████████████████████████████████████████████████████████████████████████████████████████████████████████   | 10519/10810 [1:58:40<03:05,  1.57it 97%|████████████████████████████████████████████████████████████████████████████████████████████████████████████   | 10520/10810 [1:58:40<03:04,  1.57it 97%|████████████████████████████████████████████████████████████████████████████████████████████████████████████   | 10521/10810 [1:58:41<03:03,  1.57it 97%|████████████████████████████████████████████████████████████████████████████████████████████████████████████   | 10522/10810 [1:58:42<03:03,  1.57it 97%|████████████████████████████████████████████████████████████████████████████████████████████████████████████   | 10523/10810 [1:58:42<03:02,  1.57it 97%|████████████████████████████████████████████████████████████████████████████████████████████████████████████   | 10524/10810 [1:58:43<03:01,  1.57it 97%|████████████████████████████████████████████████████████████████████████████████████████████████████████████   | 10525/10810 [1:58:44<03:01,  1.57it 97%|████████████████████████████████████████████████████████████████████████████████████████████████████████████   | 10526/10810 [1:58:44<03:00,  1.57it 97%|████████████████████████████████████████████████████████████████████████████████████████████████████████████   | 10527/10810 [1:58:45<03:00,  1.57it 97%|████████████████████████████████████████████████████████████████████████████████████████████████████████████   | 10528/10810 [1:58:45<02:59,  1.57it 97%|████████████████████████████████████████████████████████████████████████████████████████████████████████████   | 10529/10810 [1:58:46<02:58,  1.57it 97%|████████████████████████████████████████████████████████████████████████████████████████████████████████████   | 10530/10810 [1:58:47<02:58,  1.57it 97%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▏  | 10531/10810 [1:58:47<02:57,  1.57it 97%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▏  | 10532/10810 [1:58:48<02:56,  1.57it 97%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▏  | 10533/10810 [1:58:49<02:56,  1.57it 97%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▏  | 10534/10810 [1:58:49<02:55,  1.57it 97%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▏  | 10535/10810 [1:58:50<02:55,  1.57it 97%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▏  | 10536/10810 [1:58:51<02:54,  1.57it 97%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▏  | 10537/10810 [1:58:51<02:53,  1.57it 97%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▏  | 10538/10810 [1:58:52<02:52,  1.57it 97%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▏  | 10539/10810 [1:58:52<02:52,  1.57it 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▏  | 10540/10810 [1:58:53<02:51,  1.57it 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▏  | 10541/10810 [1:58:54<02:51,  1.57it 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▏  | 10542/10810 [1:58:54<02:50,  1.57it 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▎  | 10543/10810 [1:58:55<02:50,  1.57it 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▎  | 10544/10810 [1:58:56<02:49,  1.57it 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▎  | 10545/10810 [1:58:56<02:49,  1.57it 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▎  | 10546/10810 [1:58:57<02:48,  1.57it 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▎  | 10547/10810 [1:58:58<02:47,  1.57it 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▎  | 10548/10810 [1:58:58<02:47,  1.57it 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▎  | 10549/10810 [1:58:59<02:46,  1.57it 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▎  | 10550/10810 [1:58:59<02:45,  1.57it 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▎  | 10551/10810 [1:59:00<02:44,  1.57it 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▎  | 10552/10810 [1:59:01<02:44,  1.57it 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▎  | 10553/10810 [1:59:01<02:43,  1.57it 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▎  | 10554/10810 [1:59:02<02:43,  1.57it 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▍  | 10555/10810 [1:59:03<02:42,  1.57it 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▍  | 10556/10810 [1:59:03<02:41,  1.57it 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▍  | 10557/10810 [1:59:04<02:41,  1.57it 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▍  | 10558/10810 [1:59:05<02:40,  1.57it 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▍  | 10559/10810 [1:59:05<02:39,  1.57it 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▍  | 10560/10810 [1:59:06<02:39,  1.57it 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▍  | 10561/10810 [1:59:06<02:38,  1.57it 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▍  | 10562/10810 [1:59:07<02:38,  1.57it 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▍  | 10563/10810 [1:59:08<02:37,  1.57it 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▍  | 10564/10810 [1:59:08<02:36,  1.57it 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▍  | 10565/10810 [1:59:09<02:36,  1.57it 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▍  | 10566/10810 [1:59:10<02:35,  1.57it 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▌  | 10567/10810 [1:59:10<02:34,  1.57it 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▌  | 10568/10810 [1:59:11<02:34,  1.57it 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▌  | 10569/10810 [1:59:12<02:33,  1.57it 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▌  | 10570/10810 [1:59:12<02:33,  1.57it 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▌  | 10571/10810 [1:59:13<02:32,  1.57it 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▌  | 10572/10810 [1:59:13<02:31,  1.57it 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▌  | 10573/10810 [1:59:14<02:30,  1.57it 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▌  | 10574/10810 [1:59:15<02:30,  1.57it 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▌  | 10575/10810 [1:59:15<02:29,  1.57it 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▌  | 10576/10810 [1:59:16<02:29,  1.57it 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▌  | 10577/10810 [1:59:17<02:28,  1.57it 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▌  | 10578/10810 [1:59:17<02:27,  1.57it 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▋  | 10579/10810 [1:59:18<02:27,  1.57it 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▋  | 10580/10810 [1:59:19<02:26,  1.57it 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▋  | 10581/10810 [1:59:19<02:25,  1.57it 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▋  | 10582/10810 [1:59:20<02:25,  1.57it 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▋  | 10583/10810 [1:59:20<02:24,  1.57it 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▋  | 10584/10810 [1:59:21<02:23,  1.57it 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▋  | 10585/10810 [1:59:22<02:23,  1.57it 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▋  | 10586/10810 [1:59:22<02:22,  1.57it 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▋  | 10587/10810 [1:59:23<02:21,  1.57it 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▋  | 10588/10810 [1:59:24<02:21,  1.57it 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▋  | 10589/10810 [1:59:24<02:20,  1.57it 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▋  | 10590/10810 [1:59:25<02:20,  1.57it 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▊  | 10591/10810 [1:59:26<02:19,  1.57it 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▊  | 10592/10810 [1:59:26<02:18,  1.57it 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▊  | 10593/10810 [1:59:27<02:18,  1.57it 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▊  | 10594/10810 [1:59:27<02:17,  1.57it 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▊  | 10595/10810 [1:59:28<02:16,  1.57it 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▊  | 10596/10810 [1:59:29<02:16,  1.57it 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▊  | 10597/10810 [1:59:29<02:15,  1.57it 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▊  | 10598/10810 [1:59:30<02:15,  1.57it 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▊  | 10599/10810 [1:59:31<02:14,  1.57it 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▊  | 10600/10810 [1:59:31<02:13,  1.57it 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▊  | 10601/10810 [1:59:32<02:13,  1.57it 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▊  | 10602/10810 [1:59:33<02:12,  1.57it 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▊  | 10603/10810 [1:59:33<02:11,  1.57it 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▉  | 10604/10810 [1:59:34<02:11,  1.57it 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▉  | 10605/10810 [1:59:34<02:10,  1.57it 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▉  | 10606/10810 [1:59:35<02:09,  1.57it 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▉  | 10607/10810 [1:59:36<02:09,  1.57it 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▉  | 10608/10810 [1:59:36<02:08,  1.57it 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▉  | 10609/10810 [1:59:37<02:07,  1.57it 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▉  | 10610/10810 [1:59:38<02:07,  1.57it 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▉  | 10611/10810 [1:59:38<02:06,  1.57it 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▉  | 10612/10810 [1:59:39<02:06,  1.57it 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▉  | 10613/10810 [1:59:40<02:05,  1.57it 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▉  | 10614/10810 [1:59:40<02:04,  1.57it 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▉  | 10615/10810 [1:59:41<02:04,  1.57it 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████  | 10616/10810 [1:59:41<02:03,  1.57it 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████  | 10617/10810 [1:59:42<02:02,  1.57it 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████  | 10618/10810 [1:59:43<02:02,  1.57it 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████  | 10619/10810 [1:59:43<02:01,  1.57it 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████  | 10620/10810 [1:59:44<02:00,  1.57it 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████  | 10621/10810 [1:59:45<02:00,  1.57it 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████  | 10622/10810 [1:59:45<01:59,  1.57it 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████  | 10623/10810 [1:59:46<01:59,  1.57it 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████  | 10624/10810 [1:59:47<01:58,  1.57it 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████  | 10625/10810 [1:59:47<01:57,  1.57it 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████  | 10626/10810 [1:59:48<01:57,  1.57it 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████  | 10627/10810 [1:59:48<01:56,  1.57it 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▏ | 10628/10810 [1:59:49<01:55,  1.57it 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▏ | 10629/10810 [1:59:50<01:55,  1.57it 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▏ | 10630/10810 [1:59:50<01:54,  1.57it 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▏ | 10631/10810 [1:59:51<01:54,  1.57it 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▏ | 10632/10810 [1:59:52<01:53,  1.57it 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▏ | 10633/10810 [1:59:52<01:52,  1.57it 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▏ | 10634/10810 [1:59:53<01:52,  1.57it 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▏ | 10635/10810 [1:59:54<01:51,  1.57it 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▏ | 10636/10810 [1:59:54<01:50,  1.57it 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▏ | 10637/10810 [1:59:55<01:50,  1.57it 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▏ | 10638/10810 [1:59:55<01:49,  1.57it 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▏ | 10639/10810 [1:59:56<01:49,  1.57it 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▎ | 10640/10810 [1:59:57<01:48,  1.57it 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▎ | 10641/10810 [1:59:57<01:47,  1.57it 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▎ | 10642/10810 [1:59:58<01:47,  1.57it 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▎ | 10643/10810 [1:59:59<01:46,  1.57it 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▎ | 10644/10810 [1:59:59<01:45,  1.57it 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▎ | 10645/10810 [2:00:00<01:45,  1.57it 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▎ | 10646/10810 [2:00:01<01:44,  1.57it 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▎ | 10647/10810 [2:00:01<01:43,  1.57it 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▎ | 10648/10810 [2:00:02<01:43,  1.57it 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▎ | 10649/10810 [2:00:02<01:42,  1.57it 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▎ | 10650/10810 [2:00:03<01:41,  1.57it 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▎ | 10651/10810 [2:00:04<01:41,  1.57it 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▍ | 10652/10810 [2:00:04<01:40,  1.57it 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▍ | 10653/10810 [2:00:05<01:39,  1.57it 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▍ | 10654/10810 [2:00:06<01:39,  1.57it 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▍ | 10655/10810 [2:00:06<01:38,  1.57it 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▍ | 10656/10810 [2:00:07<01:38,  1.57it 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▍ | 10657/10810 [2:00:08<01:37,  1.57it 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▍ | 10658/10810 [2:00:08<01:36,  1.57it 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▍ | 10659/10810 [2:00:09<01:36,  1.57it 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▍ | 10660/10810 [2:00:10<01:35,  1.57it 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▍ | 10661/10810 [2:00:10<01:34,  1.57it 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▍ | 10662/10810 [2:00:11<01:34,  1.57it 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▍ | 10663/10810 [2:00:11<01:33,  1.57it 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▌ | 10664/10810 [2:00:12<01:32,  1.57it 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▌ | 10665/10810 [2:00:13<01:32,  1.57it 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▌ | 10666/10810 [2:00:13<01:31,  1.57it 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▌ | 10667/10810 [2:00:14<01:31,  1.57it 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▌ | 10668/10810 [2:00:15<01:30,  1.57it 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▌ | 10669/10810 [2:00:15<01:29,  1.57it 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▌ | 10670/10810 [2:00:16<01:29,  1.57it 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▌ | 10671/10810 [2:00:17<01:28,  1.57it 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▌ | 10672/10810 [2:00:17<01:27,  1.57it 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▌ | 10673/10810 [2:00:18<01:27,  1.57it 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▌ | 10674/10810 [2:00:18<01:26,  1.57it 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▌ | 10675/10810 [2:00:19<01:26,  1.57it 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▌ | 10676/10810 [2:00:20<01:25,  1.57it 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▋ | 10677/10810 [2:00:20<01:24,  1.57it 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▋ | 10678/10810 [2:00:21<01:24,  1.57it 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▋ | 10679/10810 [2:00:22<01:23,  1.57it 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▋ | 10680/10810 [2:00:22<01:22,  1.57it 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▋ | 10681/10810 [2:00:23<01:22,  1.57it 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▋ | 10682/10810 [2:00:24<01:21,  1.57it 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▋ | 10683/10810 [2:00:24<01:21,  1.57it 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▋ | 10684/10810 [2:00:25<01:20,  1.57it 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▋ | 10685/10810 [2:00:25<01:19,  1.57it 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▋ | 10686/10810 [2:00:26<01:19,  1.57it 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▋ | 10687/10810 [2:00:27<01:18,  1.57it 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▋ | 10688/10810 [2:00:27<01:17,  1.57it 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▊ | 10689/10810 [2:00:28<01:17,  1.57it 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▊ | 10690/10810 [2:00:29<01:16,  1.57it 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▊ | 10691/10810 [2:00:29<01:15,  1.57it 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▊ | 10692/10810 [2:00:30<01:15,  1.57it 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▊ | 10693/10810 [2:00:31<01:14,  1.57it 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▊ | 10694/10810 [2:00:31<01:13,  1.57it 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▊ | 10695/10810 [2:00:32<01:13,  1.57it 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▊ | 10696/10810 [2:00:32<01:12,  1.57it 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▊ | 10697/10810 [2:00:33<01:11,  1.57it 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▊ | 10698/10810 [2:00:34<01:11,  1.57it 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▊ | 10699/10810 [2:00:34<01:10,  1.57it 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▊ | 10700/10810 [2:00:35<01:10,  1.57it 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▉ | 10701/10810 [2:00:36<01:09,  1.57it 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▉ | 10702/10810 [2:00:36<01:08,  1.57it 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▉ | 10703/10810 [2:00:37<01:08,  1.57it 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▉ | 10704/10810 [2:00:38<01:07,  1.57it 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▉ | 10705/10810 [2:00:38<01:06,  1.57it 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▉ | 10706/10810 [2:00:39<01:06,  1.57it 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▉ | 10707/10810 [2:00:39<01:05,  1.57it 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▉ | 10708/10810 [2:00:40<01:04,  1.57it 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▉ | 10709/10810 [2:00:41<01:04,  1.57it 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▉ | 10710/10810 [2:00:41<01:03,  1.57it 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▉ | 10711/10810 [2:00:42<01:03,  1.57it 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▉ | 10712/10810 [2:00:43<01:02,  1.57it 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ | 10713/10810 [2:00:43<01:01,  1.57it 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ | 10714/10810 [2:00:44<01:01,  1.57it 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ | 10715/10810 [2:00:45<01:00,  1.57it 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ | 10716/10810 [2:00:45<00:59,  1.57it 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ | 10717/10810 [2:00:46<00:59,  1.57it 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ | 10718/10810 [2:00:46<00:58,  1.57it 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ | 10719/10810 [2:00:47<00:57,  1.57it 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ | 10720/10810 [2:00:48<00:57,  1.57it 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ | 10721/10810 [2:00:48<00:56,  1.57it 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ | 10722/10810 [2:00:49<00:56,  1.57it 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ | 10723/10810 [2:00:50<00:55,  1.57it 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ | 10724/10810 [2:00:50<00:54,  1.57it 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▏| 10725/10810 [2:00:51<00:54,  1.57it 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▏| 10726/10810 [2:00:52<00:53,  1.57it 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▏| 10727/10810 [2:00:52<00:52,  1.57it 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▏| 10728/10810 [2:00:53<00:52,  1.57it 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▏| 10729/10810 [2:00:53<00:51,  1.57it 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▏| 10730/10810 [2:00:54<00:51,  1.57it 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▏| 10731/10810 [2:00:55<00:50,  1.57it 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▏| 10732/10810 [2:00:55<00:49,  1.57it 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▏| 10733/10810 [2:00:56<00:49,  1.57it 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▏| 10734/10810 [2:00:57<00:48,  1.57it 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▏| 10735/10810 [2:00:57<00:47,  1.57it 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▏| 10736/10810 [2:00:58<00:47,  1.57it 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▎| 10737/10810 [2:00:59<00:46,  1.57it 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▎| 10738/10810 [2:00:59<00:45,  1.57it 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▎| 10739/10810 [2:01:00<00:45,  1.57it 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▎| 10740/10810 [2:01:00<00:44,  1.57it 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▎| 10741/10810 [2:01:01<00:43,  1.57it 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▎| 10742/10810 [2:01:02<00:43,  1.57it 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▎| 10743/10810 [2:01:02<00:42,  1.57it 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▎| 10744/10810 [2:01:03<00:42,  1.57it 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▎| 10745/10810 [2:01:04<00:41,  1.57it 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▎| 10746/10810 [2:01:04<00:40,  1.57it 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▎| 10747/10810 [2:01:05<00:40,  1.57it 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▎| 10748/10810 [2:01:06<00:39,  1.57it 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▎| 10749/10810 [2:01:06<00:38,  1.57it 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 10750/10810 [2:01:07<00:38,  1.57it 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 10751/10810 [2:01:07<00:37,  1.57it 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 10752/10810 [2:01:08<00:36,  1.57it 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 10753/10810 [2:01:09<00:36,  1.57it 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 10754/10810 [2:01:09<00:35,  1.57it 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 10755/10810 [2:01:10<00:35,  1.57it100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 10756/10810 [2:01:11<00:34,  1.57it100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 10757/10810 [2:01:11<00:33,  1.57it100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 10758/10810 [2:01:12<00:33,  1.57it100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 10759/10810 [2:01:13<00:32,  1.57it100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 10760/10810 [2:01:13<00:31,  1.57it100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 10761/10810 [2:01:14<00:31,  1.57it100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▌| 10762/10810 [2:01:14<00:30,  1.57it100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▌| 10763/10810 [2:01:15<00:29,  1.57it100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▌| 10764/10810 [2:01:16<00:29,  1.57it100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▌| 10765/10810 [2:01:16<00:28,  1.57it100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▌| 10766/10810 [2:01:17<00:28,  1.57it100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▌| 10767/10810 [2:01:18<00:27,  1.57it100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▌| 10768/10810 [2:01:18<00:26,  1.57it100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▌| 10769/10810 [2:01:19<00:26,  1.57it100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▌| 10770/10810 [2:01:20<00:25,  1.57it100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▌| 10771/10810 [2:01:20<00:24,  1.57it100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▌| 10772/10810 [2:01:21<00:24,  1.57it100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▌| 10773/10810 [2:01:22<00:23,  1.57it100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▋| 10774/10810 [2:01:22<00:22,  1.57it100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▋| 10775/10810 [2:01:23<00:22,  1.57it100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▋| 10776/10810 [2:01:23<00:21,  1.57it100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▋| 10777/10810 [2:01:24<00:21,  1.57it100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▋| 10778/10810 [2:01:25<00:20,  1.57it100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▋| 10779/10810 [2:01:25<00:19,  1.57it100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▋| 10780/10810 [2:01:26<00:19,  1.57it100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▋| 10781/10810 [2:01:27<00:18,  1.57it100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▋| 10782/10810 [2:01:27<00:17,  1.57it100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▋| 10783/10810 [2:01:28<00:17,  1.57it100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▋| 10784/10810 [2:01:29<00:16,  1.57it100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▋| 10785/10810 [2:01:29<00:15,  1.57it100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▊| 10786/10810 [2:01:30<00:15,  1.57it100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▊| 10787/10810 [2:01:30<00:14,  1.57it100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▊| 10788/10810 [2:01:31<00:14,  1.57it100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▊| 10789/10810 [2:01:32<00:13,  1.57it100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▊| 10790/10810 [2:01:32<00:12,  1.57it100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▊| 10791/10810 [2:01:33<00:12,  1.57it100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▊| 10792/10810 [2:01:34<00:11,  1.57it100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▊| 10793/10810 [2:01:34<00:10,  1.57it100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▊| 10794/10810 [2:01:35<00:10,  1.57it100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▊| 10795/10810 [2:01:36<00:09,  1.57it100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▊| 10796/10810 [2:01:36<00:08,  1.57it100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▊| 10797/10810 [2:01:37<00:08,  1.57it100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▉| 10798/10810 [2:01:37<00:07,  1.57it100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▉| 10799/10810 [2:01:38<00:07,  1.57it100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▉| 10800/10810 [2:01:39<00:06,  1.57it100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▉| 10801/10810 [2:01:39<00:05,  1.57it100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▉| 10802/10810 [2:01:40<00:05,  1.57it100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▉| 10803/10810 [2:01:41<00:04,  1.57it100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▉| 10804/10810 [2:01:41<00:03,  1.57it100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▉| 10805/10810 [2:01:42<00:03,  1.57it100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▉| 10806/10810 [2:01:43<00:02,  1.57it100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▉| 10807/10810 [2:01:43<00:01,  1.57it100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▉| 10808/10810 [2:01:44<00:01,  1.57it100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▉| 10809/10810 [2:01:44<00:00,  1.57it100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10810/10810 [2:01:45<00:00,  1.81it/s]***** Running Evaluation *****
  Num examples = 1675
  Batch size = 8
                                                                                                                                                         {'eval_loss': 0.6789549589157104, 'eval_accuracy': 0.9325373134328359, 'eval_f1 score': 0.5670498084291188, 'eval_runtime': 44.0722, 'eval_samples_per_second': 38.006, 'eval_steps_per_second': 4.765, 'epoch': 10.0}                                                                                            
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10810/10810 [2:02:29<00:00,  1.81it/s]                                                                                                                                                     Saving model checkpoint to Deberta/checkpoint-10810                                                                                                       
Configuration saved in Deberta/checkpoint-10810/config.json


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from Deberta/checkpoint-2162 (score: 0.615916955017301).
                                                                                                                                                         {'train_runtime': 7406.3183, 'train_samples_per_second': 11.671, 'train_steps_per_second': 1.46, 'train_loss': 0.06353049986833356, 'epoch': 10.0}
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10810/10810 [2:02:31<00:00,  1.81it100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10810/10810 [2:02:31<00:00,  1.47it/s]
Saving model checkpoint to ./models/saved_model_data_augmentation/
Configuration saved in ./models/saved_model_data_augmentation/config.json
loading configuration file ./models/saved_model_data_augmentation/config.json
Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "architectures": [
    "DebertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "torch_dtype": "float32",
  "transformers_version": "4.21.3",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

***** Running Evaluation *****
  Num examples = 1675
  Batch size = 8
/notebooks/DeBERTa_with_data_augmentation.py:67: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 210/210 [00:43<00:00,  4.80it/s]
{'eval_loss': 0.21063341200351715, 'eval_accuracy': 0.9337313432835821, 'eval_f1 score': 0.615916955017301, 'eval_runtime': 43.9843, 'eval_samples_per_second': 38.082, 'eval_steps_per_second': 4.774, 'epoch': 10.0}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                  eval/accuracy ▁█▅▆▅▄▅▆▆▇█
wandb:                  eval/f1 score ▁█▆▇▇▇▇▇▇▇█
wandb:                      eval/loss ▂▁▃▅▆▇▇███▁
wandb:                   eval/runtime ▃██▅▅▆▅▆▇▇▁
wandb:        eval/samples_per_second ▆▁▁▄▄▃▄▃▂▂█
wandb:          eval/steps_per_second ▆▁▁▅▄▃▄▃▂▂█
wandb:                    train/epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇████
wandb:              train/global_step ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇████
wandb:            train/learning_rate ██▇▇▇▆▆▆▅▅▅▄▄▃▃▃▂▂▂▁▁
wandb:                     train/loss █▇▆▆▄▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb:               train/total_flos ▁
wandb:               train/train_loss ▁
wandb:            train/train_runtime ▁
wandb: train/train_samples_per_second ▁
wandb:   train/train_steps_per_second ▁
wandb: 
wandb: Run summary:
wandb:                  eval/accuracy 0.93373
wandb:                  eval/f1 score 0.61592
wandb:                      eval/loss 0.21063
wandb:                   eval/runtime 43.9843
wandb:        eval/samples_per_second 38.082
wandb:          eval/steps_per_second 4.774
wandb:                    train/epoch 10.0
wandb:              train/global_step 10810
wandb:            train/learning_rate 0.0
wandb:                     train/loss 0.0016
wandb:               train/total_flos 2.650229647761408e+16
wandb:               train/train_loss 0.06353
wandb:            train/train_runtime 7406.3183
wandb: train/train_samples_per_second 11.671
wandb:   train/train_steps_per_second 1.46
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /notebooks/wandb/offline-run-20230222_180332-27mtft22
wandb: Find logs at: ./wandb/offline-run-20230222_180332-27mtft22/logs